{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyM9fsnsDfmFVdbnIRPyajYA",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/lIlIlIIIlIIl/Toy_Project/blob/main/100_sports_classification_with_ResNet.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###(사전 작업)kaggle.json 파일 다운로드 후 업로드"
      ],
      "metadata": {
        "id": "Tfxw1HLTMC4D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir -p ~/.kaggle\n",
        "!cp kaggle.json ~/.kaggle\n",
        "!chmod 600 ~/.kaggle/kaggle.json"
      ],
      "metadata": {
        "id": "023bSPLVGnzr"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!kaggle datasets download -d gpiosenka/sports-classification"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fd0ZcvPtIv3b",
        "outputId": "c52b23b8-305f-4c61-ebd2-e29800e166e4"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset URL: https://www.kaggle.com/datasets/gpiosenka/sports-classification\n",
            "License(s): CC0-1.0\n",
            "Downloading sports-classification.zip to /content\n",
            "100% 422M/424M [00:23<00:00, 20.1MB/s]\n",
            "100% 424M/424M [00:23<00:00, 18.7MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import zipfile\n",
        "zip_file = zipfile.ZipFile('/content/sports-classification.zip') # 압축을 해제할 '/파일경로/파일명.zip'\n",
        "zip_file.extractall('/content/sports-classification') # 압축을 해제할 '/위치경로/'"
      ],
      "metadata": {
        "id": "r6sABsqsLEgJ"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Import"
      ],
      "metadata": {
        "id": "rFfuXppgBuyH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets\n",
        "from torch.utils.data import DataLoader\n",
        "import argparse\n",
        "import numpy as np\n",
        "import time\n",
        "from copy import deepcopy # Add Deepcopy for args\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "dB4lHsyoBufG"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Dataset"
      ],
      "metadata": {
        "id": "e-C83h6v-tzO"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "hj3368fzvmNb"
      },
      "outputs": [],
      "source": [
        "# ===== transform 선언 ===== #\n",
        "transform = transforms.Compose([transforms.ToTensor(),\n",
        "                                transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
        "\n",
        "\n",
        "# ===== 파일이 다운로드 된 경로 ===== #\n",
        "data_folder = \"/content/sports-classification\"\n",
        "\n",
        "train_folder = data_folder + \"/train\"\n",
        "val_folder = data_folder + \"/valid\"\n",
        "test_folder = data_folder + \"/test\"\n",
        "\n",
        "trainset = datasets.ImageFolder(root=train_folder, transform=transform)\n",
        "valset = datasets.ImageFolder(root=val_folder, transform=transform)\n",
        "testset = datasets.ImageFolder(root=test_folder, transform=transform)\n",
        "\n",
        "partition = {'train': trainset, 'val':valset, 'test':testset}\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model Code"
      ],
      "metadata": {
        "id": "7BuoZjMTvs2Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cfg = {\n",
        "    'VGG11': [64, 'M', 128, 'M', 256, 256, 'M', 512, 512, 'M', 512, 512, 'M'],\n",
        "    'VGG13': [64, 64, 'M', 128, 128, 'M', 256, 256, 'M', 512, 512, 'M', 512, 512, 'M'],\n",
        "    'VGG16': [64, 64, 'M', 128, 128, 'M', 256, 256, 256, 'M', 512, 512, 512, 'M', 512, 512, 512, 'M'],\n",
        "    'VGG19': [64, 64, 'M', 128, 128, 'M', 256, 256, 256, 256, 'M', 512, 512, 512, 512, 'M', 512, 512, 512, 512, 'M'],\n",
        "}"
      ],
      "metadata": {
        "id": "YWZrYZ7CvvZm"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# CNN Model"
      ],
      "metadata": {
        "id": "E8k1koXyvwe6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class CNN(nn.Module):\n",
        "    def __init__(self, model_code, in_channels, kernel_size, stride, out_dim, hid_dim, n_linear, act_func, use_bn, dropout):\n",
        "        super(CNN, self).__init__()\n",
        "        self.model_code = model_code\n",
        "        self.in_channels = in_channels\n",
        "        self.kernel_size = kernel_size\n",
        "        self.stride = stride\n",
        "        self.out_dim = out_dim\n",
        "        self.hid_dim = hid_dim\n",
        "        self.act_func = act_func\n",
        "        self.use_bn = use_bn\n",
        "        self.n_linear = n_linear\n",
        "        self.Dropout = nn.Dropout(dropout)\n",
        "        self.padding = self.kernel_size // 2\n",
        "        self.data_size = 224 # 모듈화 필요\n",
        "\n",
        "\n",
        "        if (32 - self.kernel_size + 2*self.padding)/self.stride + 1 != 32:\n",
        "            raise ValueError('filter is not valid')\n",
        "\n",
        "\n",
        "        if self.act_func == 'relu':\n",
        "            self.act_func = nn.ReLU()\n",
        "\n",
        "        elif self.act_func == 'tanh':\n",
        "            self.act_func = nn.Tanh()\n",
        "\n",
        "        elif self.act_func == 'sigmoid':\n",
        "            self.act_func = nn.Sigmoid()\n",
        "        else:\n",
        "            raise ValueError('no valid activation function selected!')\n",
        "\n",
        "        self.layers = self._make_layers()\n",
        "        self.classifier = self._make_fc_layers()\n",
        "\n",
        "    def _make_layers(self):\n",
        "        layers = [] # Sequential을 사용하기 위해 ModuleList가 아닌 그냥 List 사용\n",
        "        for x in cfg[self.model_code]:\n",
        "            if x == 'M':\n",
        "                layers += [nn.MaxPool2d(2, 2)]\n",
        "                self.data_size = self.data_size // 2 # 풀링 진행할 때마다 데이터 사이즈 절반으로 축소\n",
        "\n",
        "            else:\n",
        "                layers += [nn.Conv2d(self.in_channels, x, self.kernel_size, self.stride, self.padding)]\n",
        "                if self.use_bn:\n",
        "                    layers += [nn.BatchNorm2d(x)]\n",
        "                layers += [self.act_func]\n",
        "                self.in_channels = x # in_channels 업데이트\n",
        "\n",
        "        self.in_channels = self.in_channels * self.data_size ** 2\n",
        "\n",
        "        return nn.Sequential(*layers)\n",
        "        # *layers: layers 리스트의 데이터들을 각각 받아주는 문법(가변 인수 느낌)\n",
        "        # Sequential은 리스트를 받아서 하나의 서브모델을 만들어주는 문법. CNN모델 내의 layers모델을 서브모델로 만듦\n",
        "\n",
        "    def _make_fc_layers(self):\n",
        "        linears = []\n",
        "        linears.append(nn.Linear(self.in_channels, self.hid_dim))\n",
        "        for i in range(self.n_linear - 2):\n",
        "            linears += [self.act_func, nn.Linear(self.hid_dim, self.hid_dim),self.Dropout]\n",
        "\n",
        "        linears += [self.act_func, nn.Linear(self.hid_dim, self.out_dim),self.Dropout]\n",
        "        return nn.Sequential(*linears)\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.layers(x) # layers가 하나의 서브모델\n",
        "        x = x.view(x.size(0), -1) # batch size는 그대로 유지하면서 3차원 tensor를 벡터 형태로 펼쳐준다.\n",
        "        x = self.classifier(x)\n",
        "\n",
        "        return x"
      ],
      "metadata": {
        "id": "Pp2oyFx0v0Rc"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d06C7AiPCum-"
      },
      "source": [
        "## Dimenstion Check"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lZxGPqj9Cwii",
        "outputId": "f525eab3-9c5c-450b-839a-30a037be127b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([16, 10])\n"
          ]
        }
      ],
      "source": [
        "def dimension_check():\n",
        "    net = CNN('VGG11', 3, 3, 1, 10, 4096, 3,'relu', True, 0)\n",
        "    x = torch.randn(16, 3, 224, 224) # 해당 크기를 갖는 임의의 텐서를 생성\n",
        "    y = net(x)\n",
        "    print(y.size())\n",
        "\n",
        "# layer를 쌓을 때마다 dimension이 맞는지 체크해주는 함수\n",
        "\n",
        "dimension_check()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kscmVrLlCzrN"
      },
      "source": [
        "# Train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "WOoctoVjC3WN"
      },
      "outputs": [],
      "source": [
        "def train(net, partition, optimizer, criterion, args):\n",
        "    trainloader = torch.utils.data.DataLoader(partition['train'],\n",
        "                                              batch_size=args.train_batch_size,\n",
        "                                              shuffle=True, num_workers=2)\n",
        "    net.train()\n",
        "\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    train_loss = 0.0\n",
        "    for i, data in enumerate(trainloader, 0):\n",
        "        optimizer.zero_grad() # [21.01.05 오류 수정] 매 Epoch 마다 .zero_grad()가 실행되는 것을 매 iteration 마다 실행되도록 수정했습니다.\n",
        "\n",
        "        # get the inputs\n",
        "        inputs, labels = data\n",
        "        # print(labels)\n",
        "        # print(inputs.size()) # => (batchsize, channel, H, W) = (16, 3, 224, 224). 즉 dataloader가 4차원 텐서 형태로 데이터를 받아놓음; (shape쓰면 안됨)\n",
        "        inputs = inputs.cuda()\n",
        "        labels = labels.cuda()\n",
        "        outputs = net(inputs)\n",
        "\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        train_loss += loss.item()\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "    train_loss = train_loss / len(trainloader)\n",
        "    train_acc = 100 * correct / total\n",
        "    return net, train_loss, train_acc"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VS_q0YwXC4hf"
      },
      "source": [
        "# Validation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "VVT1C04LC5yw"
      },
      "outputs": [],
      "source": [
        "def validate(net, partition, criterion, args):\n",
        "    valloader = torch.utils.data.DataLoader(partition['val'],\n",
        "                                            batch_size=args.test_batch_size,\n",
        "                                            shuffle=False, num_workers=2)\n",
        "    net.eval()\n",
        "\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    val_loss = 0\n",
        "    with torch.no_grad():\n",
        "        for data in valloader:\n",
        "            images, labels = data\n",
        "            images = images.cuda()\n",
        "            labels = labels.cuda()\n",
        "            outputs = net(images)\n",
        "\n",
        "            loss = criterion(outputs, labels)\n",
        "\n",
        "            val_loss += loss.item()\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "\n",
        "        val_loss = val_loss / len(valloader)\n",
        "        val_acc = 100 * correct / total\n",
        "    return val_loss, val_acc"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z1qxk-LBC7yl"
      },
      "source": [
        "# Test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "S1X7QJ7bC99F"
      },
      "outputs": [],
      "source": [
        "def test(net, partition, args):\n",
        "    testloader = torch.utils.data.DataLoader(partition['test'],\n",
        "                                             batch_size=args.test_batch_size,\n",
        "                                             shuffle=False, num_workers=2)\n",
        "    net.eval()\n",
        "\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for data in testloader:\n",
        "            images, labels = data\n",
        "            images = images.cuda()\n",
        "            labels = labels.cuda()\n",
        "\n",
        "            outputs = net(images)\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "\n",
        "        test_acc = 100 * correct / total\n",
        "    return test_acc"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5FYfQCc2DBjF"
      },
      "source": [
        "# Experiment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "id": "kvo2-xj3DCrP"
      },
      "outputs": [],
      "source": [
        "def experiment(partition, args):\n",
        "\n",
        "    net = CNN(args.model_code, args.in_channels, args.kernel_size, args.stride, args.out_dim, args.hid_dim, args.n_linear ,args.act_func, args.use_bn, args.dropout)\n",
        "    net.cuda()\n",
        "\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    if args.optim == 'SGD':\n",
        "        optimizer = optim.SGD(net.parameters(), lr=args.lr, weight_decay=args.l2)\n",
        "    elif args.optim == 'RMSprop':\n",
        "        optimizer = optim.RMSprop(net.parameters(), lr=args.lr, weight_decay=args.l2)\n",
        "    elif args.optim == 'Adam':\n",
        "        optimizer = optim.Adam(net.parameters(), lr=args.lr, weight_decay=args.l2)\n",
        "    else:\n",
        "        raise ValueError('In-valid optimizer choice')\n",
        "\n",
        "    train_losses = []\n",
        "    val_losses = []\n",
        "    train_accs = []\n",
        "    val_accs = []\n",
        "\n",
        "    for epoch in range(args.epoch):  # loop over the dataset multiple times\n",
        "        ts = time.time()\n",
        "        net, train_loss, train_acc = train(net, partition, optimizer, criterion, args)\n",
        "        val_loss, val_acc = validate(net, partition, criterion, args)\n",
        "        te = time.time()\n",
        "\n",
        "        train_losses.append(train_loss)\n",
        "        val_losses.append(val_loss)\n",
        "        train_accs.append(train_acc)\n",
        "        val_accs.append(val_acc)\n",
        "\n",
        "        print('Epoch {}, Acc(train/val): {:2.2f}/{:2.2f}, Loss(train/val) {:2.2f}/{:2.2f}. Took {:2.2f} sec'.format(epoch, train_acc, val_acc, train_loss, val_loss, te-ts))\n",
        "\n",
        "    test_acc = test(net, partition, args)\n",
        "\n",
        "    result = {}\n",
        "    result['train_losses'] = train_losses\n",
        "    result['val_losses'] = val_losses\n",
        "    result['train_accs'] = train_accs\n",
        "    result['val_accs'] = val_accs\n",
        "    result['train_acc'] = train_acc\n",
        "    result['val_acc'] = val_acc\n",
        "    result['test_acc'] = test_acc\n",
        "    return vars(args), result"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import hashlib\n",
        "import json\n",
        "from os import listdir\n",
        "from os.path import isfile, join\n",
        "import pandas as pd\n",
        "\n",
        "def save_exp_result(setting, result):\n",
        "    exp_name = setting['exp_name']\n",
        "    del setting['epoch']\n",
        "    del setting['test_batch_size']\n",
        "\n",
        "    hash_key = hashlib.sha1(str(setting).encode()).hexdigest()[:6]\n",
        "    filename = './results/{}-{}.json'.format(exp_name, hash_key)\n",
        "    result.update(setting)\n",
        "    with open(filename, 'w') as f:\n",
        "        json.dump(result, f)\n",
        "\n",
        "\n",
        "def load_exp_result(exp_name):\n",
        "    dir_path = './results'\n",
        "    filenames = [f for f in listdir(dir_path) if isfile(join(dir_path, f)) if '.json' in f]\n",
        "    list_result = []\n",
        "    for filename in filenames:\n",
        "        if exp_name in filename:\n",
        "            with open(join(dir_path, filename), 'r') as infile:\n",
        "                results = json.load(infile)\n",
        "                list_result.append(results)\n",
        "    df = pd.DataFrame(list_result) # .drop(columns=[])\n",
        "    return df"
      ],
      "metadata": {
        "id": "YP03HcxI_tqe"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ====== Random Seed Initialization ====== #\n",
        "seed = 123\n",
        "np.random.seed(seed)\n",
        "torch.manual_seed(seed)\n",
        "\n",
        "parser = argparse.ArgumentParser()\n",
        "args = parser.parse_args(\"\")\n",
        "args.exp_name = \"exp8\"\n",
        "\n",
        "# ====== Model Capacity ====== #\n",
        "args.out_dim = 100\n",
        "args.hid_dim = 512\n",
        "args.n_linear = 2\n",
        "args.act_func = 'relu'\n",
        "args.model_code = 'VGG13'\n",
        "args.in_channels = 3\n",
        "args.kernel_size = 3\n",
        "args.stride = 1\n",
        "\n",
        "# ====== Regularization ======= #\n",
        "args.l2 = 0.0001\n",
        "args.use_bn = True\n",
        "args.dropout = 0.3\n",
        "\n",
        "# ====== Optimizer & Training ====== #\n",
        "args.optim = 'Adam' #'RMSprop' #SGD, RMSprop, ADAM...\n",
        "args.lr = 0.001\n",
        "args.epoch = 15\n",
        "\n",
        "args.train_batch_size = 32\n",
        "args.test_batch_size = 32\n",
        "\n",
        "# ====== Experiment Variable ====== #\n",
        "name_var1 = 'act_func'\n",
        "name_var2 = 'optim'\n",
        "list_var1 = ['relu', 'tanh', 'sigmoid']\n",
        "list_var2 = ['SGD', 'RMSprop', 'Adam']\n",
        "\n",
        "\n",
        "for var1 in list_var1:\n",
        "    for var2 in list_var2:\n",
        "        setattr(args, name_var1, var1)\n",
        "        setattr(args, name_var2, var2)\n",
        "        print(args)\n",
        "\n",
        "        setting, result = experiment(partition, deepcopy(args))\n",
        "        save_exp_result(setting, result)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 611
        },
        "id": "pLXr3vr0wyq0",
        "outputId": "ea8d807b-1631-4535-9caf-7701c00e3f28"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Namespace(exp_name='exp8', out_dim=100, hid_dim=512, n_linear=2, act_func='relu', model_code='VGG13', in_channels=3, kernel_size=3, stride=1, l2=0.0001, use_bn=True, dropout=0.3, optim='SGD', lr=0.001, epoch=15, train_batch_size=32, test_batch_size=32)\n",
            "Epoch 0, Acc(train/val): 6.70/15.00, Loss(train/val) 4.37/3.96. Took 181.79 sec\n",
            "Epoch 1, Acc(train/val): 17.31/25.20, Loss(train/val) 3.82/3.43. Took 181.31 sec\n",
            "Epoch 2, Acc(train/val): 25.56/32.80, Loss(train/val) 3.44/3.04. Took 181.43 sec\n",
            "Epoch 3, Acc(train/val): 31.81/40.40, Loss(train/val) 3.16/2.79. Took 181.69 sec\n",
            "Epoch 4, Acc(train/val): 36.91/37.60, Loss(train/val) 2.92/2.61. Took 182.00 sec\n",
            "Epoch 5, Acc(train/val): 40.85/49.00, Loss(train/val) 2.76/2.42. Took 181.84 sec\n",
            "Epoch 6, Acc(train/val): 44.90/48.40, Loss(train/val) 2.59/2.28. Took 181.43 sec\n",
            "Epoch 7, Acc(train/val): 47.61/54.00, Loss(train/val) 2.48/2.13. Took 181.42 sec\n",
            "Epoch 8, Acc(train/val): 50.39/58.00, Loss(train/val) 2.36/2.02. Took 181.32 sec\n",
            "Epoch 9, Acc(train/val): 53.26/54.40, Loss(train/val) 2.22/1.94. Took 181.63 sec\n",
            "Epoch 10, Acc(train/val): 55.40/58.20, Loss(train/val) 2.13/1.92. Took 181.98 sec\n",
            "Epoch 11, Acc(train/val): 57.55/61.40, Loss(train/val) 2.05/1.81. Took 181.72 sec\n",
            "Epoch 12, Acc(train/val): 59.80/58.20, Loss(train/val) 1.94/1.83. Took 181.44 sec\n",
            "Epoch 13, Acc(train/val): 61.73/61.20, Loss(train/val) 1.86/1.75. Took 181.53 sec\n",
            "Epoch 14, Acc(train/val): 62.90/56.60, Loss(train/val) 1.79/1.77. Took 183.22 sec\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: './results/exp8-1c4b81.json'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-48-4300de98c3f1>\u001b[0m in \u001b[0;36m<cell line: 40>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m         \u001b[0msetting\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mexperiment\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpartition\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdeepcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 47\u001b[0;31m         \u001b[0msave_exp_result\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msetting\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-42-72d2bdf65e5d>\u001b[0m in \u001b[0;36msave_exp_result\u001b[0;34m(setting, result)\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0mfilename\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'./results/{}-{}.json'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexp_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhash_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msetting\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'w'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m         \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './results/exp8-1c4b81.json'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(torch.cuda.memory_summary(device=None, abbreviated=False))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3a_coxkDyM5x",
        "outputId": "2ec66fac-9150-4cbc-b1e7-632f565d44fc"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "|===========================================================================|\n",
            "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
            "|---------------------------------------------------------------------------|\n",
            "|            CUDA OOMs: 7            |        cudaMalloc retries: 7         |\n",
            "|===========================================================================|\n",
            "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocated memory      |  14955 MiB |  14955 MiB |  14956 MiB |    883 KiB |\n",
            "|       from large pool |  14942 MiB |  14942 MiB |  14942 MiB |      0 KiB |\n",
            "|       from small pool |     13 MiB |     13 MiB |     14 MiB |    883 KiB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active memory         |  14955 MiB |  14955 MiB |  14956 MiB |    883 KiB |\n",
            "|       from large pool |  14942 MiB |  14942 MiB |  14942 MiB |      0 KiB |\n",
            "|       from small pool |     13 MiB |     13 MiB |     14 MiB |    883 KiB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Requested memory      |  14949 MiB |  14949 MiB |  14950 MiB |    882 KiB |\n",
            "|       from large pool |  14936 MiB |  14936 MiB |  14936 MiB |      0 KiB |\n",
            "|       from small pool |     13 MiB |     13 MiB |     14 MiB |    882 KiB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved memory   |  14968 MiB |  14968 MiB |  14968 MiB |      0 B   |\n",
            "|       from large pool |  14954 MiB |  14954 MiB |  14954 MiB |      0 B   |\n",
            "|       from small pool |     14 MiB |     14 MiB |     14 MiB |      0 B   |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable memory |  12825 KiB |  32016 KiB | 174220 KiB | 161395 KiB |\n",
            "|       from large pool |  12288 KiB |  30368 KiB | 165152 KiB | 152864 KiB |\n",
            "|       from small pool |    537 KiB |   2041 KiB |   9068 KiB |   8531 KiB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocations           |     495    |     495    |     498    |       3    |\n",
            "|       from large pool |      46    |      46    |      46    |       0    |\n",
            "|       from small pool |     449    |     449    |     452    |       3    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active allocs         |     495    |     495    |     498    |       3    |\n",
            "|       from large pool |      46    |      46    |      46    |       0    |\n",
            "|       from small pool |     449    |     449    |     452    |       3    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved segments |      26    |      26    |      26    |       0    |\n",
            "|       from large pool |      19    |      19    |      19    |       0    |\n",
            "|       from small pool |       7    |       7    |       7    |       0    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable allocs |       9    |      11    |      20    |      11    |\n",
            "|       from large pool |       6    |       7    |      13    |       7    |\n",
            "|       from small pool |       3    |       5    |       7    |       4    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize allocations  |       0    |       0    |       0    |       0    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize GPU segments |       0    |       0    |       0    |       0    |\n",
            "|===========================================================================|\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_acc(var1, var2, df):\n",
        "\n",
        "    fig, ax = plt.subplots(1, 3)\n",
        "    fig.set_size_inches(15, 6)\n",
        "    sns.set_style(\"darkgrid\", {\"axes.facecolor\": \".9\"})\n",
        "\n",
        "    sns.barplot(x=var1, y='train_acc', hue=var2, data=df, ax=ax[0])\n",
        "    sns.barplot(x=var1, y='val_acc', hue=var2, data=df, ax=ax[1])\n",
        "    sns.barplot(x=var1, y='test_acc', hue=var2, data=df, ax=ax[2])\n",
        "\n",
        "    ax[0].set_title('Train Accuracy')\n",
        "    ax[1].set_title('Validation Accuracy')\n",
        "    ax[2].set_title('Test Accuracy')\n",
        "\n",
        "\n",
        "def plot_loss_variation(var1, var2, df, **kwargs):\n",
        "\n",
        "    list_v1 = df[var1].unique()\n",
        "    list_v2 = df[var2].unique()\n",
        "    list_data = []\n",
        "\n",
        "    for value1 in list_v1:\n",
        "        for value2 in list_v2:\n",
        "            row = df.loc[df[var1]==value1]\n",
        "            row = row.loc[df[var2]==value2]\n",
        "\n",
        "            train_losses = list(row.train_losses)[0]\n",
        "            val_losses = list(row.val_losses)[0]\n",
        "\n",
        "            for epoch, train_loss in enumerate(train_losses):\n",
        "                list_data.append({'type':'train', 'loss':train_loss, 'epoch':epoch, var1:value1, var2:value2})\n",
        "            for epoch, val_loss in enumerate(val_losses):\n",
        "                list_data.append({'type':'val', 'loss':val_loss, 'epoch':epoch, var1:value1, var2:value2})\n",
        "\n",
        "    df = pd.DataFrame(list_data)\n",
        "    g = sns.FacetGrid(df, row=var2, col=var1, hue='type', **kwargs)\n",
        "    g = g.map(plt.plot, 'epoch', 'loss', marker='.')\n",
        "    g.add_legend()\n",
        "    g.fig.suptitle('Train loss vs Val loss')\n",
        "    plt.subplots_adjust(top=0.89) # 만약 Title이 그래프랑 겹친다면 top 값을 조정해주면 됩니다! 함수 인자로 받으면 그래프마다 조절할 수 있겠죠?\n",
        "\n",
        "\n",
        "def plot_acc_variation(var1, var2, df, **kwargs):\n",
        "    list_v1 = df[var1].unique()\n",
        "    list_v2 = df[var2].unique()\n",
        "    list_data = []\n",
        "\n",
        "    for value1 in list_v1:\n",
        "        for value2 in list_v2:\n",
        "            row = df.loc[df[var1]==value1]\n",
        "            row = row.loc[df[var2]==value2]\n",
        "\n",
        "            train_accs = list(row.train_accs)[0]\n",
        "            val_accs = list(row.val_accs)[0]\n",
        "            test_acc = list(row.test_acc)[0]\n",
        "\n",
        "            for epoch, train_acc in enumerate(train_accs):\n",
        "                list_data.append({'type':'train', 'Acc':train_acc, 'test_acc':test_acc, 'epoch':epoch, var1:value1, var2:value2})\n",
        "            for epoch, val_acc in enumerate(val_accs):\n",
        "                list_data.append({'type':'val', 'Acc':val_acc, 'test_acc':test_acc, 'epoch':epoch, var1:value1, var2:value2})\n",
        "\n",
        "    df = pd.DataFrame(list_data)\n",
        "    g = sns.FacetGrid(df, row=var2, col=var1, hue='type', **kwargs)\n",
        "    g = g.map(plt.plot, 'epoch', 'Acc', marker='.')\n",
        "\n",
        "    def show_acc(x, y, metric, **kwargs):\n",
        "        plt.scatter(x, y, alpha=0.3, s=1)\n",
        "        metric = \"Test Acc: {:1.3f}\".format(list(metric.values)[0])\n",
        "        plt.text(0.05, 0.95, metric,  horizontalalignment='left', verticalalignment='center', transform=plt.gca().transAxes, bbox=dict(facecolor='yellow', alpha=0.5, boxstyle=\"round,pad=0.1\"))\n",
        "    g = g.map(show_acc, 'epoch', 'Acc', 'test_acc')\n",
        "\n",
        "    g.add_legend()\n",
        "    g.fig.suptitle('Train Accuracy vs Val Accuracy')\n",
        "    plt.subplots_adjust(top=0.89)"
      ],
      "metadata": {
        "id": "qYHZAUX6K5Ns"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "var1 = 'act_func'\n",
        "var2 = 'optim'\n",
        "df = load_exp_result('exp8')\n",
        "\n",
        "plot_acc(var1, var2, df)\n",
        "plot_loss_variation(var1, var2, df, sharey=False) #sharey를 True로 하면 모둔 subplot의 y축의 스케일이 같아집니다.\n",
        "plot_acc_variation(var1, var2, df, margin_titles=True, sharey=True) #margin_titles를 True로 하면 그래프의 가장자리에 var1과 var2 값이 표시되고 False로 하면 각 subplot 위에 표시됩니다."
      ],
      "metadata": {
        "id": "clHNWLkmLAIq"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}